{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"📂 Verifying current path:\", os.getcwd())\n",
        "\n",
        "daily_path = \"data/daily\"\n",
        "if not os.path.exists(daily_path):\n",
        "    daily_path = \"../data/daily\"\n",
        "\n",
        "print(f\"\\n📁 Scanning path: {daily_path}\")\n",
        "if os.path.exists(daily_path):\n",
        "    print(\"📄 Files found:\")\n",
        "    for f in sorted(os.listdir(daily_path)):\n",
        "        print(\"  -\", f)\n",
        "else:\n",
        "    print(\"❌ Path does not exist.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Q82cKx5pm4g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4P7f7KrZR-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98efd7d7-a211-4e55-dad1-7d4cc07e5985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Missing daily files:\n",
            "  - MLB_Combined_Odds_Results_2025-05-02.csv\n",
            "  - MLB_Combined_Odds_Results_2025-05-03.csv\n",
            "  - MLB_Combined_Odds_Results_2025-05-04.csv\n",
            "  - MLB_Combined_Odds_Results_2025-05-05.csv\n",
            "  - MLB_Combined_Odds_Results_2025-05-06.csv\n",
            "  - MLB_Combined_Odds_Results_2025-05-10.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# === Config ===\n",
        "folder = \"data/daily\"\n",
        "start_date = datetime(2025, 5, 7)\n",
        "end_date = datetime.today()\n",
        "\n",
        "missing = []\n",
        "\n",
        "for i in range((end_date - start_date).days + 1):\n",
        "    day = start_date + timedelta(days=i)\n",
        "    filename = f\"MLB_Combined_Odds_Results_{day.strftime('%Y-%m-%d')}.csv\"\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    if not os.path.exists(full_path):\n",
        "        missing.append(filename)\n",
        "\n",
        "if missing:\n",
        "    print(\"❌ Missing daily files:\")\n",
        "    for f in missing:\n",
        "        print(\"  -\", f)\n",
        "else:\n",
        "    print(\"✅ All expected files are present!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# === Config ===\n",
        "yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "daily_file = f\"../data/daily/MLB_Combined_Odds_Results_{yesterday}.csv\"\n",
        "abbrev_file = \"../data/lookups/MLB_Teams_Template_2025.xlsx\"\n",
        "master_file = \"../data/master/master_template.parquet\"\n",
        "\n",
        "# === Verify paths ===\n",
        "print(\"📂 Verifying current path:\", os.getcwd())\n",
        "for root, dirs, files in os.walk(\"data/daily\"):\n",
        "    print(f\"\\n📁 {root}\")\n",
        "    for f in files:\n",
        "        print(f\"  └── {f}\")\n",
        "\n",
        "# === Guard Clause ===\n",
        "if not os.path.exists(daily_file):\n",
        "    print(f\"⚠️ No daily file found: {daily_file} — skipping append.\")\n",
        "    exit()\n",
        "\n",
        "# === Load game-level data and team abbreviations ===\n",
        "df = pd.read_csv(daily_file)\n",
        "abbrev_df = pd.read_excel(abbrev_file).rename(columns={\"City and Team\": \"team_name\", \"Abbreviation\": \"team_abbr\"})\n",
        "abbrev_map = dict(zip(abbrev_df[\"team_name\"], abbrev_df[\"team_abbr\"]))\n",
        "\n",
        "# 🔧 Normalize game_date to 'YYYY-MM-DD' format to match master\n",
        "df['game_date'] = pd.to_datetime(df['game_date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "# === Transform to team-level rows ===\n",
        "team_rows = []\n",
        "for _, row in df.iterrows():\n",
        "    if pd.isna(row[\"home_score\"]) or pd.isna(row[\"away_score\"]):\n",
        "        continue\n",
        "    total_score = row[\"home_score\"] + row[\"away_score\"]\n",
        "    hit_over = total_score > row[\"total_line\"] if pd.notna(row[\"total_line\"]) else None\n",
        "\n",
        "    for team_type in [\"home\", \"away\"]:\n",
        "        is_home = team_type == \"home\"\n",
        "        team = row[f\"{team_type}_team\"]\n",
        "        opponent = row[f\"{'away' if is_home else 'home'}_team\"]\n",
        "        team_score = row[f\"{team_type}_score\"]\n",
        "        opp_score = row[f\"{'away' if is_home else 'home'}_score\"]\n",
        "        moneyline = row[f\"moneyline_{team_type}\"]\n",
        "\n",
        "        row_data = {\n",
        "            \"game_id\": row[\"game_id\"],\n",
        "            \"game_date_et\": pd.to_datetime(row[\"game_date\"]),\n",
        "            \"start_time_et\": pd.to_datetime(row[\"start_time_et\"]),\n",
        "            \"team\": team,\n",
        "            \"team_abbr\": abbrev_map.get(team),\n",
        "            \"opponent\": opponent,\n",
        "            \"opponent_abbr\": abbrev_map.get(opponent),\n",
        "            \"is_home\": is_home,\n",
        "            \"home_score\": row[\"home_score\"],\n",
        "            \"away_score\": row[\"away_score\"],\n",
        "            \"run_diff\": team_score - opp_score,\n",
        "            \"won_game\": team_score > opp_score,\n",
        "            \"hit_over\": hit_over,\n",
        "            \"merge_key\": f\"{team}_{row['game_date']}\",\n",
        "            \"h2h_own\": moneyline,  # 🔥 Updated to align with master\n",
        "            \"h2h_opp\": row[f\"moneyline_{'away' if is_home else 'home'}\"],  # 🔥 Updated to align with master\n",
        "            \"is_home_odds\": is_home,\n",
        "            \"Run_Line\": None, \"Spread_Price\": None, \"Opp_Spread_Price\": None, \"Total\": row[\"total_line\"],\n",
        "            \"Over_Price\": row[\"over_odds\"], \"Under_Price\": row[\"under_odds\"],\n",
        "            \"team_abbr_odds\": abbrev_map.get(team), \"opponent_abbr_odds\": abbrev_map.get(opponent)\n",
        "        }\n",
        "        for i in range(1, 10):\n",
        "            row_data[f\"home_{i}\"] = row.get(f\"home_{i}\")\n",
        "            row_data[f\"away_{i}\"] = row.get(f\"away_{i}\")\n",
        "        team_rows.append(row_data)\n",
        "\n",
        "team_df = pd.DataFrame(team_rows)\n",
        "\n",
        "# === Ensure numeric consistency for odds fields ===\n",
        "for col in [\"h2h_own\", \"h2h_opp\", \"Over_Price\", \"Under_Price\", \"Total\", \"run_diff\"]:\n",
        "    team_df[col] = pd.to_numeric(team_df[col], errors=\"coerce\")\n",
        "\n",
        "# === Append to master Parquet (safe for upsert logic) ===\n",
        "if os.path.exists(master_file):\n",
        "    master_df = pd.read_parquet(master_file)\n",
        "    master_df[\"game_date_et\"] = pd.to_datetime(master_df[\"game_date_et\"], errors=\"coerce\")\n",
        "    master_df[\"start_time_et\"] = pd.to_datetime(master_df[\"start_time_et\"], errors=\"coerce\")\n",
        "    master_df = master_df[~master_df['game_id'].isin(team_df['game_id'])]  # 🔥 Optional upsert to avoid duplicate game_id\n",
        "    combined_df = pd.concat([master_df, team_df], ignore_index=True)\n",
        "else:\n",
        "    combined_df = team_df\n",
        "\n",
        "# ✅ Optional: Add season\n",
        "combined_df[\"season\"] = pd.to_datetime(combined_df[\"game_date_et\"], errors='coerce').dt.year\n",
        "\n",
        "# ✅ Optional: Sort by date and team\n",
        "combined_df = combined_df.sort_values(by=[\"game_date_et\", \"team_abbr\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# === Save updated master ===\n",
        "combined_df.to_parquet(master_file, index=False)\n",
        "print(f\"✅ Appended {len(team_df)} team-level rows to: {master_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s54o0SoeTEDh",
        "outputId": "8ff2b5ef-fdd3-4c26-abf1-82d2817d3645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Appended 30 team-level rows to: master_template.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-261eb1546d29>:83: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat([master_df, team_df], ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmv7SOzuxQCp",
        "outputId": "df0832da-cc55-419e-e453-09e1b192fc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}